{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementa√ß√£o de c√≥digo baseado no artigo de Hannah Levin, Samir Agarwala e Juan Triana: Understanding Complex Emotions in Sentences\n",
        "\n",
        "### Atividade referente √† disciplina Processamento de Linguagem Natural, com a Professora N√°dia F√©lix\n",
        "\n",
        "*Alunas: Karla Riccioppo e L√≠via Oliveira.*\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "> Explica√ß√µes detalhadas de cada c√©lula\n",
        "\n",
        "> **1¬™ c√©lula**\n",
        "\n",
        "Esta c√©lula √© crucial para configurar o ambiente do projeto. Ela realiza as seguintes a√ß√µes:\n",
        "  1. Importa√ß√£o de Bibliotecas: Importa todas as bibliotecas Python que ser√£o usadas nas etapas seguintes do projeto. Isso inclui:\n",
        "      * torch, torch.nn, accelerate: Para deep learning e computa√ß√£o com GPUs.\n",
        "      * transformers, BertTokenizer, BertModel: Essenciais para trabalhar com modelos pr√©-treinados como o BERT, que √© a base do seu modelo de segmenta√ß√£o de emo√ß√µes.\n",
        "      * json, re, requests, numpy, tqdm, accuracy_score, collections.Counter: Para manipula√ß√£o de dados, express√µes regulares, requisi√ß√µes HTTP, opera√ß√µes num√©ricas, barras de progresso, avalia√ß√£o de modelos e contagem de ocorr√™ncias, respectivamente.\n",
        "  2. Inicializa√ß√£o do Dispositivo: Verifica se uma GPU (placa de v√≠deo) est√° dispon√≠vel no ambiente (torch.cuda.is_available()). Se estiver, o modelo ser√° executado na GPU para aproveitar o processamento paralelo e acelerar o treinamento. Caso contr√°rio, ele usar√° a CPU. Isso √© armazenado na vari√°vel device.\n",
        "  3. Confirma√ß√£o do Dispositivo: Imprime o dispositivo que ser√° utilizado (üéØ Dispositivo: cpu no seu caso, pois a sa√≠da indica cpu) e, se for uma GPU, tamb√©m informa o nome dela. Esta sa√≠da ajuda a confirmar que o ambiente est√° configurado corretamente.\n",
        "\n"
      ],
      "metadata": {
        "id": "89pZcTZ3UY4n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LElycX9fqlwH",
        "outputId": "562ac27b-329b-455c-c124-066ab75d79d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "üéØ Dispositivo: cpu\n"
          ]
        }
      ],
      "source": [
        "#1. importa√ß√µes e inicializa√ß√£o do modelo\n",
        "!pip install transformers torch accelerate tqdm requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import json\n",
        "import re\n",
        "import requests\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from collections import Counter\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üéØ Dispositivo: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üöÄ GPU: {torch.cuda.get_device_name()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **2¬™ c√©lula**\n",
        "\n",
        "Esta c√©lula √© respons√°vel por carregar e fazer uma an√°lise inicial do seu dataset de emo√ß√µes. Veja o que ela faz:\n",
        "1. Carregamento do Dataset do GitHub: Ela primeiro imprime uma mensagem de \"Carregando dataset do GitHub...\" e ent√£o usa a biblioteca requests para baixar um arquivo JSON de uma URL espec√≠fica no GitHub. Este arquivo cont√©m as frases e suas segmenta√ß√µes emocionais.\n",
        "2. Confirma√ß√£o do Carregamento: Ap√≥s o download, ela imprime uma mensagem confirmando que o dataset foi carregado com sucesso e quantos exemplos ele cont√©m (no seu caso, 307 exemplos).\n",
        "3. Exibi√ß√£o de Exemplos: Para ajudar a visualizar a estrutura dos dados, a c√©lula mostra os tr√™s primeiros exemplos do dataset. Para cada exemplo, ela exibe a frase original e o segmented_text, que √© a mesma frase com as tags de emo√ß√£o (<EMOCAO-Inicio>, <EMOCAO-Fim>) inseridas.\n",
        "4. An√°lise da Distribui√ß√£o das Emo√ß√µes: Este √© um passo importante para entender a composi√ß√£o do seu dataset. Ela faz o seguinte:\n",
        "    * Usa uma express√£o regular (re.findall(r'<([A-Z_]+)-Inicio>', item['segmented_text'])) para encontrar todas as tags de in√≠cio de emo√ß√£o (como <FELIZ-Inicio>, <MEDO-Inicio>) em cada segmented_text.\n",
        "    * A biblioteca collections.Counter √© usada para contar a frequ√™ncia de cada emo√ß√£o encontrada. Aten√ß√£o: Na √∫ltima intera√ß√£o, corrigi a express√£o regular para Inicio (sem acento) para corresponder ao formato real do dataset, o que resolveu o problema de sa√≠da incompleta que voc√™ mencionou.\n",
        "    * Finalmente, calcula a porcentagem de cada emo√ß√£o em rela√ß√£o ao total de segmentos emocionais e imprime um resumo da distribui√ß√£o (ex.: FELIZ: 149 segmentos (23.8%), etc.).\n",
        "\n",
        "Em resumo, esta c√©lula prepara os dados, mostrando o que voc√™ est√° trabalhando e dando uma vis√£o geral da frequ√™ncia de cada emo√ß√£o no seu conjunto de dados."
      ],
      "metadata": {
        "id": "ywzoUT1gYxc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#2. carregando o dataset do reposit√≥rio do github\n",
        "print(\"üì• Carregando dataset do GitHub...\")\n",
        "github_url = \"https://raw.githubusercontent.com/liviafernanda/pln-inf-ufg/refs/heads/main/hannah_dataset_pt-br.json\"\n",
        "\n",
        "try:\n",
        "    response = requests.get(github_url)\n",
        "    data = response.json()\n",
        "    print(f\"‚úÖ Dataset carregado: {len(data)} exemplos\")\n",
        "\n",
        "    for item in data[:3]:  # Mostrar primeiros 3 exemplos\n",
        "        print(f\"\\nüìù Exemplo \" + str(data.index(item) + 1) + \":\")\n",
        "        print(f\"   Frase: {item['sentence']}\")\n",
        "        print(f\"   Segmentado: {item['segmented_text']}\")\n",
        "\n",
        "    # An√°lise da distribui√ß√£o\n",
        "    print(\"\\nüìä ANALISANDO DISTRIBUI√á√ÉO DAS EMO√á√ïES:\")\n",
        "    emotion_counts = Counter()\n",
        "    for item in data:\n",
        "        emotions = re.findall(r'<([A-Z_]+)-Inicio>', item['segmented_text'])\n",
        "        emotion_counts.update(emotions)\n",
        "\n",
        "    total_segments = sum(emotion_counts.values())\n",
        "    for emotion, count in emotion_counts.most_common():\n",
        "        percentage = (count / total_segments) * 100\n",
        "        print(f\"   {emotion}: {count} segmentos ({percentage:.1f}%)\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Erro: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGiugVnAquO0",
        "outputId": "b1b54125-4e80-476d-e415-ea9fa3f538c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• Carregando dataset do GitHub...\n",
            "‚úÖ Dataset carregado: 307 exemplos\n",
            "\n",
            "üìù Exemplo 1:\n",
            "   Frase: Depois de semanas tentando resolver o problema, o c√≥digo finalmente rodou sem erros, e por um instante tudo pareceu fazer sentido, mas ent√£o percebi que o resultado n√£o correspondia ao esperado.\n",
            "   Segmentado: <FELIZ-Inicio>Depois de semanas tentando resolver o problema, o c√≥digo finalmente rodou sem erros, e por um instante tudo pareceu fazer sentido<FELIZ-Fim><TRISTE-Inicio>, mas ent√£o percebi que o resultado n√£o correspondia ao esperado.<TRISTE-Fim>\n",
            "\n",
            "üìù Exemplo 2:\n",
            "   Frase: As crian√ßas corriam pelo parque, rindo alto e espalhando folhas pelo ch√£o, at√© que o trov√£o estrondou e todos correram para se abrigar.\n",
            "   Segmentado: <FELIZ-Inicio>As crian√ßas corriam pelo parque, rindo alto e espalhando folhas pelo ch√£o<FELIZ-Fim><MEDO-Inicio>, at√© que o trov√£o estrondou e todos correram para se abrigar.<MEDO-Fim>\n",
            "\n",
            "üìù Exemplo 3:\n",
            "   Frase: Eu esperava encontrar a casa como a deixei, mas ao abrir a porta, tudo estava vazio e o eco das minhas lembran√ßas foi a √∫nica resposta.\n",
            "   Segmentado: <NEUTRO-Inicio>Eu esperava encontrar a casa como a deixei<NEUTRO-Fim><TRISTE-Inicio>, mas ao abrir a porta, tudo estava vazio e o eco das minhas lembran√ßas foi a √∫nica resposta.<TRISTE-Fim>\n",
            "\n",
            "üìä ANALISANDO DISTRIBUI√á√ÉO DAS EMO√á√ïES:\n",
            "   FELIZ: 149 segmentos (23.8%)\n",
            "   NEUTRO: 137 segmentos (21.9%)\n",
            "   MEDO: 112 segmentos (17.9%)\n",
            "   SURPRESA: 85 segmentos (13.6%)\n",
            "   TRISTE: 82 segmentos (13.1%)\n",
            "   RAIVA: 62 segmentos (9.9%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **3¬™ c√©lula**\n",
        "\n",
        "Esta c√©lula define a classe EmotionDataset, que √© fundamental para preparar seus dados brutos de texto para o modelo BERT. Ela faz o seguinte:\n",
        "1. Prop√≥sito da Classe (EmotionDataset): Esta classe herda de torch.utils.data.Dataset, o que significa que ela foi projetada para interagir com o DataLoader do PyTorch. O objetivo √© transformar cada frase do seu dataset em um formato num√©rico que o modelo BERT possa entender, ou seja, input_ids (tokens num√©ricos), attention_mask (para indicar o que √© texto real e o que √© preenchimento) e labels (as emo√ß√µes correspondentes a cada token).\n",
        "2. Inicializa√ß√£o (__init__):\n",
        "    * Recebe os dados brutos (data), o tokenizer do BERT e um max_length (tamanho m√°ximo das sequ√™ncias).\n",
        "    * Define mapeamentos entre nomes de emo√ß√µes e IDs num√©ricos (emotion_to_id) e vice-versa (id_to_emotion). Isso √© crucial porque os modelos de Machine Learning trabalham com n√∫meros, n√£o com strings.\n",
        "    * Imprime o mapeamento das emo√ß√µes para que voc√™ possa ver qual n√∫mero corresponde a cada emo√ß√£o.\n",
        "    * Chama o m√©todo _process_data() para pr√©-processar todo o dataset assim que a classe √© instanciada.\n",
        "3. Processamento dos Dados (_process_data):\n",
        "    * Este m√©todo itera sobre cada item (frase) do seu dataset original.\n",
        "    * Para cada frase, ele usa o tokenizer para convert√™-la em input_ids (IDs dos tokens), attention_mask (m√°scara de aten√ß√£o) e aplica padding (preenchimento) ou truncation (corte) para que todas as sequ√™ncias tenham o max_length definido.\n",
        "    * Em seguida, ele chama _extract_token_labels() para obter as etiquetas de emo√ß√£o para cada token na frase.\n",
        "    * Armazena os input_ids, attention_mask e labels de cada frase em uma lista processed.\n",
        "4. Extra√ß√£o de Etiquetas de Tokens (_extract_token_labels):\n",
        "    * Esta √© a parte mais complexa: o objetivo √© atribuir uma emo√ß√£o a CADA token da frase.\n",
        "    * Come√ßa inicializando todas as etiquetas com 0 (que corresponde a NEUTRO).\n",
        "    * Usa uma express√£o regular (r'<([A-Z_]+)-Inicio>(.*?)< -Fim>') para encontrar os segmentos de texto marcados com emo√ß√µes (ex: <FELIZ-Inicio>...<FELIZ-Fim>) no segmented_text.\n",
        "    * Para cada segmento emocional encontrado:\n",
        "        * Tokeniza o texto desse segmento.\n",
        "        * Procura a sequ√™ncia desses tokens dentro da lista completa de tokens da frase (obtida do tokenizer).\n",
        "        * Uma vez encontrada a correspond√™ncia, ele atribui o emotion_id (n√∫mero da emo√ß√£o) correspondente a todos os tokens desse segmento.\n",
        "    * Tratamento de Tokens Especiais: Tokens como [CLS] (in√≠cio da sequ√™ncia), [SEP] (separador de sequ√™ncia) e [PAD] (preenchimento) n√£o devem contribuir para o c√°lculo da perda do modelo. Por isso, suas etiquetas s√£o definidas como -100, que √© um valor especial que o CrossEntropyLoss (fun√ß√£o de perda) ignora por padr√£o.\n",
        "    5. M√©todos __len__ e __getitem__:\n",
        "    * __ len__: Retorna o n√∫mero total de exemplos processados no dataset, permitindo que o DataLoader saiba quantos itens existem.\n",
        "    * __ getitem__: Permite acessar um item espec√≠fico do dataset pelo seu √≠ndice, o que √© essencial para que o DataLoader possa carregar os dados em lotes (batches).\n",
        "    \n",
        "Em resumo, esta classe √© a ponte entre seus dados de texto brutos e o formato num√©rico estruturado que o modelo BERT-LSTM espera para treinamento, garantindo que cada token tenha uma etiqueta de emo√ß√£o correspondente."
      ],
      "metadata": {
        "id": "4GUx4cs2aD1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#3. pre processamento do dataseet\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=128):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.emotion_to_id = {\n",
        "            'NEUTRO': 0, 'FELIZ': 1, 'TRISTE': 2,\n",
        "            'RAIVA': 3, 'MEDO': 4, 'SURPRESA': 5\n",
        "        }\n",
        "        self.id_to_emotion = {v: k for k, v in self.emotion_to_id.items()}\n",
        "\n",
        "        print(\"üé≠ Mapeamento de emo√ß√µes:\")\n",
        "        for emotion, eid in self.emotion_to_id.items():\n",
        "            print(f\"   {emotion} -> {eid}\")\n",
        "\n",
        "        self.processed_data = self._process_data()\n",
        "\n",
        "    def _process_data(self):\n",
        "        processed = []\n",
        "        print(\"üîÑ Processando dados...\")\n",
        "\n",
        "        for item in tqdm(self.data, desc=\"Processando\"):\n",
        "            sentence = item['sentence']\n",
        "            segmented_text = item['segmented_text']\n",
        "\n",
        "            encoding = self.tokenizer(\n",
        "                sentence,\n",
        "                max_length=self.max_length,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            input_ids = encoding['input_ids'][0]\n",
        "            tokens = self.tokenizer.convert_ids_to_tokens(input_ids)\n",
        "            token_labels = self._extract_token_labels(tokens, segmented_text)\n",
        "\n",
        "            processed.append({\n",
        "                'input_ids': encoding['input_ids'][0],\n",
        "                'attention_mask': encoding['attention_mask'][0],\n",
        "                'labels': torch.tensor(token_labels, dtype=torch.long)\n",
        "            })\n",
        "\n",
        "        return processed\n",
        "\n",
        "    def _extract_token_labels(self, tokens, segmented_text):\n",
        "        labels = [0] * len(tokens)  # NEUTRO como padr√£o\n",
        "\n",
        "        try:\n",
        "            pattern = r'<([A-Z_]+)-Inicio>(.*?)<\\1-Fim>'\n",
        "            matches = re.findall(pattern, segmented_text)\n",
        "\n",
        "            for emotion, text in matches:\n",
        "                text = text.strip()\n",
        "                segment_tokens = self.tokenizer.tokenize(text)\n",
        "\n",
        "                # Buscar o segmento na senten√ßa\n",
        "                for i in range(len(tokens) - len(segment_tokens) + 1):\n",
        "                    if tokens[i:i+len(segment_tokens)] == segment_tokens:\n",
        "                        emotion_id = self.emotion_to_id.get(emotion, 0)\n",
        "                        for j in range(len(segment_tokens)):\n",
        "                            if i + j < len(labels):\n",
        "                                labels[i + j] = emotion_id\n",
        "                        break\n",
        "\n",
        "            # Marcar tokens especiais\n",
        "            for i, token in enumerate(tokens):\n",
        "                if token in [self.tokenizer.cls_token, self.tokenizer.sep_token, self.tokenizer.pad_token]:\n",
        "                    labels[i] = -100\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Erro no processamento: {e}\")\n",
        "\n",
        "        return labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.processed_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.processed_data[idx]"
      ],
      "metadata": {
        "id": "h7gzpycqrBVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **4¬™ c√©lula**\n",
        "\n",
        "Esta c√©lula define a arquitetura do seu modelo de Segmenta√ß√£o de Emo√ß√µes, chamado EmotionSegmentationModel. Ele √© uma combina√ß√£o de um modelo BERT pr√©-treinado com uma camada LSTM e um classificador. Vamos detalhar cada parte:\n",
        "1. Inicializa√ß√£o (__init__):\n",
        "      * super(EmotionSegmentationModel, self).__ init__(): Chama o construtor da classe pai nn.Module.\n",
        "      * print(\"üß† Carregando BERT com fine-tuning...\"): Indica o in√≠cio do carregamento do modelo BERT.\n",
        "      * self.bert = BertModel.from_pretrained(bert_model_name): Carrega um modelo BERT pr√©-treinado. No seu caso, ele usa o neuralmind/bert-base-portuguese-cased, que √© um BERT base treinado para o portugu√™s.\n",
        "2. Fine-tuning Parcial do BERT:\n",
        "      * for name, param in self.bert.named_parameters():: Itera sobre todos os par√¢metros (pesos e vieses) do modelo BERT.\n",
        "      * if any(layer in name for layer in ['layer.10', 'layer.11', 'pooler']):: Esta condi√ß√£o verifica se o par√¢metro pertence √†s duas √∫ltimas camadas do BERT (layer.10, layer.11) ou √† camada pooler. Estas s√£o as camadas mais pr√≥ximas da sa√≠da do BERT e, geralmente, as mais espec√≠ficas para a tarefa de fine-tuning.\n",
        "      * param.requires_grad = True: Para essas camadas espec√≠ficas, requires_grad √© definido como True. Isso significa que os pesos dessas camadas ser√£o atualizados durante o treinamento (fine-tuning).\n",
        "      * else: param.requires_grad = False: Para as demais camadas do BERT (as mais baixas), requires_grad √© definido como False. Isso 'congela' os pesos dessas camadas, impedindo que sejam alterados durante o treinamento. Essa t√©cnica ajuda a preservar o conhecimento geral do BERT e a acelerar o treinamento, focando as atualiza√ß√µes nas camadas mais relevantes para a sua tarefa espec√≠fica.\n",
        "3. Camada LSTM Bidirecional (self.lstm):\n",
        "      * nn.LSTM(...): Ap√≥s o BERT extrair caracter√≠sticas contextuais do texto, uma camada Long Short-Term Memory (LSTM) √© adicionada. LSTMs s√£o redes neurais recorrentes eficazes para processar sequ√™ncias.\n",
        "      * input_size=self.bert.config.hidden_size: A entrada para a LSTM √© a sa√≠da de cada token do BERT (que tem hidden_size dimens√µes).\n",
        "      * hidden_size=hidden_size: Define o tamanho da sa√≠da oculta de cada unidade LSTM.\n",
        "      * num_layers=num_layers: Define quantas camadas LSTM s√£o empilhadas.\n",
        "      * bidirectional=True: Muito importante! Significa que a LSTM processa a sequ√™ncia em duas dire√ß√µes (da esquerda para a direita e da direita para a esquerda). Isso permite que o modelo capture o contexto de um token a partir de palavras que v√™m antes e depois dele, o que √© crucial para entender nuances emocionais.\n",
        "      * batch_first=True: Indica que a dimens√£o do batch (tamanho do lote de dados) √© a primeira.\n",
        "      * dropout: Ajuda a prevenir overfitting, zerando aleatoriamente algumas conex√µes durante o treinamento.\n",
        "4. Classificador Melhorado (self.classifier):\n",
        "      * nn.Sequential(...): Um m√≥dulo sequencial que organiza as camadas do classificador.\n",
        "      * nn.Dropout(dropout): Uma camada de dropout inicial para regulariza√ß√£o.\n",
        "      * nn.Linear(hidden_size * 2, hidden_size): Uma camada linear (totalmente conectada). Como a LSTM √© bidirecional, sua sa√≠da tem hidden_size * 2 dimens√µes, que s√£o mapeadas para hidden_size.\n",
        "      * nn.ReLU(): Uma fun√ß√£o de ativa√ß√£o Rectified Linear Unit, que introduz n√£o-linearidade no modelo.\n",
        "      * nn.Dropout(dropout): Outra camada de dropout para mais regulariza√ß√£o.\n",
        "      * nn.Linear(hidden_size, num_emotions): A camada de sa√≠da final. Mapeia a sa√≠da da camada anterior para o n√∫mero de classes de emo√ß√£o (num_emotions) que voc√™ tem. A sa√≠da desta camada s√£o os 'logits', que ser√£o usados para calcular as probabilidades de cada emo√ß√£o por token.\n",
        "5. Passagem forward: Este m√©todo descreve como os dados fluem atrav√©s do modelo durante a infer√™ncia ou treinamento:\n",
        "      * bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask): Primeiro, os input_ids e attention_mask s√£o passados para o modelo BERT. Ele retorna v√°rias sa√≠das, e last_hidden_state √© a sequ√™ncia de representa√ß√µes contextuais para cada token.\n",
        "      * lstm_output, _ = self.lstm(sequence_output): A sa√≠da do BERT (sequence_output) √© ent√£o alimentada √† camada LSTM bidirecional.\n",
        "      * logits = self.classifier(lstm_output): A sa√≠da da LSTM √© passada para o classificador, que produz os logits para cada token, indicando a probabilidade de cada emo√ß√£o.\n",
        "\n",
        "Em resumo, este modelo aproveita o poderoso entendimento contextual de um BERT pr√©-treinado em portugu√™s, refina esse entendimento com camadas LSTM bidirecionais para capturar depend√™ncias de longo alcance na sequ√™ncia, e ent√£o usa um classificador para prever a emo√ß√£o de cada token individualmente. A estrat√©gia de fine-tuning parcial otimiza o uso dos recursos e o tempo de treinamento."
      ],
      "metadata": {
        "id": "iu14B2GyaQWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Estrutura do modelo\n",
        "class EmotionSegmentationModel(nn.Module):\n",
        "    def __init__(self, bert_model_name, num_emotions, hidden_size=256, num_layers=2, dropout=0.3):\n",
        "        super(EmotionSegmentationModel, self).__init__()\n",
        "\n",
        "        print(\"üß† Carregando BERT com fine-tuning...\")\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "\n",
        "        # FINE-TUNING PARCIAL: descongelar √∫ltimas camadas\n",
        "        for name, param in self.bert.named_parameters():\n",
        "            if any(layer in name for layer in ['layer.10', 'layer.11', 'pooler']):\n",
        "                param.requires_grad = True\n",
        "            else:\n",
        "                param.requires_grad = False\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.bert.config.hidden_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            bidirectional=True,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        # CLASSIFICADOR MELHORADO\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size * 2, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size, num_emotions)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # COM fine-tuning nas √∫ltimas camadas\n",
        "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        sequence_output = bert_outputs.last_hidden_state\n",
        "\n",
        "        lstm_output, _ = self.lstm(sequence_output)\n",
        "        logits = self.classifier(lstm_output)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "w6Jp_lTLrjWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **5¬™ c√©lula**\n",
        "\n",
        "Esta c√©lula cont√©m as fun√ß√µes centrais para o treinamento, avalia√ß√£o e uso do seu modelo de segmenta√ß√£o de emo√ß√µes. Vamos detalhar cada uma:\n",
        "1. train_model_improved(model, train_loader, val_loader, num_epochs=10, learning_rate=0.0005, patience=3)\n",
        "Esta √© a fun√ß√£o principal de treinamento do modelo. Ela otimiza o modelo para aprender a mapear texto para emo√ß√µes. Veja o que ela faz:\n",
        "    * Otimizador (optimizer): Usa AdamW, um otimizador popular para modelos baseados em Transformers, para ajustar os pesos do modelo com base na perda.\n",
        "    * Fun√ß√£o de Perda (criterion): Emprega CrossEntropyLoss, que √© adequada para problemas de classifica√ß√£o multi-classe. O ignore_index=-100 √© importante, pois indica que tokens com label -100 (os tokens especiais como [CLS], [SEP], [PAD] que voc√™ configurou na classe EmotionDataset) n√£o contribuem para o c√°lculo da perda.\n",
        "    * Loop de Treinamento: Itera sobre o n√∫mero de num_epochs (√©pocas) definidos.\n",
        "        * Mini-batches: Dentro de cada √©poca, o modelo processa os dados em mini-batches do train_loader.\n",
        "        * Forward Pass: Os input_ids e attention_mask s√£o passados para o modelo (logits = model(input_ids, attention_mask)), que retorna as previs√µes (logits).\n",
        "        * C√°lculo da Perda: A perda √© calculada comparando os logits com os labels reais.\n",
        "        * Backward Pass e Otimiza√ß√£o: A perda √© retropropagada (loss.backward()) para calcular os gradientes, e o otimizador (optimizer.step()) atualiza os pesos do modelo.\n",
        "        * Clipping de Gradiente: torch.nn.utils.clip_grad_norm_ √© usado para evitar 'gradientes explosivos', um problema comum em redes neurais profundas.\n",
        "    * Valida√ß√£o e Early Stopping: Ap√≥s cada √©poca de treinamento, o modelo √© avaliado no conjunto de valida√ß√£o usando evaluate_model_colab. Se a acur√°cia de valida√ß√£o melhorar, o modelo atual √© salvo como o 'melhor modelo'. Se a acur√°cia n√£o melhorar por um n√∫mero de √©pocas (patience), o treinamento √© interrompido (early stopping) para evitar overfitting.\n",
        "    * Cach√™ da GPU: torch.cuda.empty_cache() libera a mem√≥ria da GPU ap√≥s cada √©poca, ajudando a evitar problemas de mem√≥ria.\n",
        "    * Carregar Melhor Modelo: Ao final do treinamento, o modelo com a melhor performance de valida√ß√£o √© carregado novamente, garantindo que voc√™ tenha a melhor vers√£o treinada.\n",
        "2. evaluate_model_colab(model, data_loader)\n",
        "Esta fun√ß√£o √© usada para avaliar o desempenho do modelo em um conjunto de dados (geralmente o de valida√ß√£o ou teste) e calcular a acur√°cia. Ela faz o seguinte:\n",
        "    * Modo de Avalia√ß√£o: Define o modelo em model.eval() para desativar recursos como Dropout, que s√≥ s√£o usados durante o treinamento.\n",
        "    * Infer√™ncia sem Gradientes: Usa torch.no_grad() para desativar o c√°lculo de gradientes, o que economiza mem√≥ria e acelera a infer√™ncia.\n",
        "    * Previs√µes: Para cada batch no data_loader, o modelo faz previs√µes e calcula as classes mais prov√°veis (torch.argmax(logits, dim=-1)).\n",
        "    * Filtragem de Labels Ignorados: As previs√µes e labels s√£o filtradas para remover os tokens com label -100 (tokens especiais).\n",
        "    * C√°lculo da Acur√°cia: accuracy_score da scikit-learn √© usado para calcular a acur√°cia geral do modelo nas previs√µes v√°lidas.\n",
        "    * Voltar ao Modo de Treinamento: Retorna o modelo para model.train() ao final da avalia√ß√£o.\n",
        "3. predict_emotion_segments_improved(model, tokenizer, sentence, id_to_emotion, max_length=128, min_confidence=0.4)\n",
        "Esta √© a fun√ß√£o que voc√™ usar√° para fazer previs√µes com o modelo treinado em uma nova frase. Ela n√£o apenas prev√™, mas tamb√©m p√≥s-processa os resultados de forma inteligente:\n",
        "    * Modo de Avalia√ß√£o: Coloca o modelo em model.eval().\n",
        "    * Tokeniza√ß√£o e Previs√£o: A frase de entrada √© tokenizada pelo tokenizer e passada para o modelo para obter os logits e as probabilities (probabilidades de cada emo√ß√£o para cada token).\n",
        "    * Extra√ß√£o de Tokens e Confian√ßa: Converte os IDs dos tokens de volta para strings e extrai as previs√µes de emo√ß√£o e as probabilidades m√°ximas para cada token.\n",
        "    * P√≥s-processamento Inteligente: Este √© o cora√ß√£o desta fun√ß√£o, projetado para criar segmentos de emo√ß√£o mais coesos e compreens√≠veis:\n",
        "        * Agrupamento de Emo√ß√µes: Itera sobre os tokens e agrupa tokens consecutivos com a mesma emo√ß√£o para formar segmentos.\n",
        "        * Confian√ßa M√≠nima (min_confidence): Se a confian√ßa de uma predi√ß√£o for abaixo de min_confidence, o modelo tenta manter a emo√ß√£o do token anterior, o que ajuda a suavizar transi√ß√µes e a ignorar 'ru√≠do' de baixa confian√ßa.\n",
        "        * Filtragem de Segmentos Pequenos: Descarta segmentos de texto muito curtos que n√£o cont√™m pelo menos 3 caracteres √∫teis.\n",
        "        * Mesclagem de Segmentos Pequenos Consecutivos: Se dois segmentos consecutivos t√™m a mesma emo√ß√£o e s√£o ambos muito curtos (menos de 2 palavras), eles s√£o mesclados em um √∫nico segmento. Isso ajuda a evitar segmenta√ß√µes excessivamente fragmentadas.\n",
        "    * Retorno dos Segmentos: Retorna uma lista de dicion√°rios, onde cada dicion√°rio representa um segmento de emo√ß√£o detectado, contendo o text (texto do segmento), emotion (emo√ß√£o atribu√≠da) e confidence (confian√ßa m√©dia da emo√ß√£o para aquele segmento).\n",
        "    \n",
        "Em resumo, essas fun√ß√µes formam o pipeline completo para treinar um modelo de segmenta√ß√£o de emo√ß√µes, monitorar seu desempenho e, em seguida, us√°-lo para analisar novas frases, incluindo um p√≥s-processamento inteligente para gerar resultados mais coerentes."
      ],
      "metadata": {
        "id": "gmGLJmzVbqm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Fun√ß√µes de treino\n",
        "def train_model_improved(model, train_loader, val_loader, num_epochs=10, learning_rate=0.0005, patience=3):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "    best_accuracy = 0\n",
        "    patience_counter = 0\n",
        "    train_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    print(f\"üöÄ Iniciando treinamento com {num_epochs} √©pocas...\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        progress_bar = tqdm(train_loader, desc=f'√âpoca {epoch+1}/{num_epochs}')\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logits = model(input_ids, attention_mask)\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            progress_bar.set_postfix({'perda': f'{loss.item():.4f}'})\n",
        "\n",
        "        epoch_loss = total_loss / len(train_loader)\n",
        "        train_losses.append(epoch_loss)\n",
        "\n",
        "        # VALIDA√á√ÉO\n",
        "        val_accuracy = evaluate_model_colab(model, val_loader)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        print(f'üìä √âpoca {epoch+1} - Perda: {epoch_loss:.4f}, Acur√°cia Val: {val_accuracy:.4f}')\n",
        "\n",
        "        # EARLY STOPPING\n",
        "        if val_accuracy > best_accuracy:\n",
        "            best_accuracy = val_accuracy\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), 'melhor_modelo.pt')\n",
        "            print(f'üíæ Melhor modelo salvo (acur√°cia: {val_accuracy:.4f})')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f'üõë Early stopping na √©poca {epoch+1}')\n",
        "                break\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Carregar melhor modelo\n",
        "    model.load_state_dict(torch.load('melhor_modelo.pt'))\n",
        "    print(f'‚úÖ Melhor modelo carregado (acur√°cia: {best_accuracy:.4f})')\n",
        "\n",
        "    return train_losses, val_accuracies\n",
        "\n",
        "def evaluate_model_colab(model, data_loader):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(data_loader, desc=\"Validando\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            logits = model(input_ids, attention_mask)\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "            mask = labels != -100\n",
        "            valid_predictions = predictions[mask].cpu().numpy()\n",
        "            valid_labels = labels[mask].cpu().numpy()\n",
        "\n",
        "            all_predictions.extend(valid_predictions)\n",
        "            all_labels.extend(valid_labels)\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_predictions) if all_labels else 0.0\n",
        "    model.train()\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "def predict_emotion_segments_improved(model, tokenizer, sentence, id_to_emotion, max_length=128, min_confidence=0.4):\n",
        "    \"\"\"Vers√£o melhorada com p√≥s-processamento inteligente\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    encoding = tokenizer(\n",
        "        sentence,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids, attention_mask)\n",
        "        probabilities = torch.softmax(logits, dim=-1)\n",
        "        max_probs, predictions = torch.max(probabilities, dim=-1)\n",
        "\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "    predictions = predictions[0].cpu().numpy()\n",
        "    max_probs = max_probs[0].cpu().numpy()\n",
        "\n",
        "    # P√ìS-PROCESSAMENTO INTELIGENTE\n",
        "    segments = []\n",
        "    current_emotion = None\n",
        "    current_tokens = []\n",
        "    current_confidences = []\n",
        "\n",
        "    for i, (token, emotion_id, confidence) in enumerate(zip(tokens, predictions, max_probs)):\n",
        "        if token in [tokenizer.cls_token, tokenizer.sep_token, tokenizer.pad_token]:\n",
        "            continue\n",
        "\n",
        "        emotion = id_to_emotion[emotion_id]\n",
        "\n",
        "        # SE CONFIAN√áA BAIXA, MANTER EMO√á√ÉO ANTERIOR\n",
        "        if confidence < min_confidence and current_emotion:\n",
        "            emotion = current_emotion\n",
        "\n",
        "        if emotion != current_emotion:\n",
        "            if current_tokens:\n",
        "                segment_text = tokenizer.convert_tokens_to_string(current_tokens).strip()\n",
        "                avg_confidence = np.mean(current_confidences) if current_confidences else 0.0\n",
        "\n",
        "                # FILTRAR SEGMENTOS MUITO PEQUENOS (menos de 2 caracteres √∫teis)\n",
        "                if len(segment_text) >= 3:\n",
        "                    segments.append({\n",
        "                        'text': segment_text,\n",
        "                        'emotion': current_emotion,\n",
        "                        'confidence': avg_confidence\n",
        "                    })\n",
        "\n",
        "            current_emotion = emotion\n",
        "            current_tokens = [token]\n",
        "            current_confidences = [confidence]\n",
        "        else:\n",
        "            current_tokens.append(token)\n",
        "            current_confidences.append(confidence)\n",
        "\n",
        "    # √öLTIMO SEGMENTO\n",
        "    if current_tokens:\n",
        "        segment_text = tokenizer.convert_tokens_to_string(current_tokens).strip()\n",
        "        avg_confidence = np.mean(current_confidences) if current_confidences else 0.0\n",
        "        if len(segment_text) >= 3:\n",
        "            segments.append({\n",
        "                'text': segment_text,\n",
        "                'emotion': current_emotion,\n",
        "                'confidence': avg_confidence\n",
        "            })\n",
        "\n",
        "    # MESCLAR SEGMENTOS PEQUENOS CONSECUTIVOS DA MESMA EMO√á√ÉO\n",
        "    if len(segments) > 1:\n",
        "        merged_segments = []\n",
        "        i = 0\n",
        "        while i < len(segments):\n",
        "            current = segments[i]\n",
        "            if i < len(segments) - 1:\n",
        "                next_seg = segments[i + 1]\n",
        "                # Se segmentos consecutivos da mesma emo√ß√£o e ambos pequenos, mesclar\n",
        "                if (current['emotion'] == next_seg['emotion'] and\n",
        "                    len(current['text'].split()) <= 2 and\n",
        "                    len(next_seg['text'].split()) <= 2):\n",
        "                    merged_text = current['text'] + ' ' + next_seg['text']\n",
        "                    merged_confidence = (current['confidence'] + next_seg['confidence']) / 2\n",
        "                    merged_segments.append({\n",
        "                        'text': merged_text.strip(),\n",
        "                        'emotion': current['emotion'],\n",
        "                        'confidence': merged_confidence\n",
        "                    })\n",
        "                    i += 2  # Pular o pr√≥ximo\n",
        "                else:\n",
        "                    merged_segments.append(current)\n",
        "                    i += 1\n",
        "            else:\n",
        "                merged_segments.append(current)\n",
        "                i += 1\n",
        "        segments = merged_segments\n",
        "\n",
        "    return segments"
      ],
      "metadata": {
        "id": "8bwUgEGxrwEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **6¬™ c√©lula**\n",
        "\n",
        "Esta c√©lula √© o cora√ß√£o do seu processo de treinamento, onde tudo o que foi definido nas c√©lulas anteriores √© juntado para treinar o modelo de segmenta√ß√£o de emo√ß√µes. Vamos ver o que acontece passo a passo:\n",
        "  1. In√≠cio do Treinamento: A c√©lula come√ßa com uma mensagem clara: \"=== üöÄ INICIANDO TREINAMENTO OTIMIZADO ===\" para indicar o come√ßo do processo.\n",
        "  2. Configura√ß√£o do BERT (bert_model_name): Define o nome do modelo BERT que ser√° utilizado. No seu caso, √© o neuralmind/bert-base-portuguese-cased, que √© uma vers√£o do BERT treinada especificamente para o portugu√™s.\n",
        "  3. Carregamento do Tokenizer: O BertTokenizer.from_pretrained(bert_model_name) carrega o tokenizador correspondente ao modelo BERT escolhido. O tokenizador √© essencial para converter seu texto em sequ√™ncias num√©ricas (tokens e IDs) que o BERT pode processar. Uma mensagem \"‚úÖ Tokenizer carregado\" confirma o sucesso.\n",
        "  4. Cria√ß√£o do Dataset: Uma inst√¢ncia da classe EmotionDataset (que definimos na c√©lula 3) √© criada, passando os dados brutos (data carregado da c√©lula 2) e o tokenizer. Esta etapa pr√©-processa todo o seu dataset, convertendo as frases e suas anota√ß√µes de emo√ß√£o em input_ids, attention_mask e labels prontos para o treinamento. Uma mensagem \"‚úÖ Dataset criado\" indica que o processamento inicial foi conclu√≠do.\n",
        "  5. Divis√£o Treino/Valida√ß√£o (train_test_split):\n",
        "      * O dataset total √© dividido em dois subconjuntos: 80% para treinamento (train_dataset) e 20% para valida√ß√£o (val_dataset).\n",
        "      * torch.utils.data.random_split garante que essa divis√£o seja feita de forma aleat√≥ria, o que √© importante para que ambos os conjuntos sejam representativos.\n",
        "      * As vari√°veis train_loader e val_loader s√£o criadas usando DataLoader. Elas s√£o respons√°veis por carregar os dados em 'lotes' (batches) para o treinamento e valida√ß√£o, respectivamente, e podem embaralhar os dados (shuffle=True para treino) para melhorar o aprendizado do modelo.\n",
        "      * Uma mensagem \"üìä Divis√£o: ... treino, ... valida√ß√£o\" mostra o tamanho de cada conjunto.\n",
        "  6. Inicializa√ß√£o do Modelo (EmotionSegmentationModel):\n",
        "      * Uma inst√¢ncia da classe EmotionSegmentationModel (definida na c√©lula 4) √© criada. S√£o passados par√¢metros como o nome do modelo BERT, o n√∫mero de emo√ß√µes (obtido do dataset.emotion_to_id), o tamanho da camada oculta da LSTM (hidden_size), o n√∫mero de camadas LSTM e o dropout.\n",
        "      * .to(device) move o modelo para o dispositivo de computa√ß√£o (CPU ou GPU) que foi detectado na primeira c√©lula, garantindo que as opera√ß√µes sejam executadas no hardware correto.\n",
        "      * A c√©lula imprime \"üß† Modelo criado\" e \"üìê Par√¢metros trein√°veis:\", mostrando quantos par√¢metros do modelo ser√£o ajustados durante o treinamento (lembre-se que algumas camadas do BERT foram congeladas).\n",
        "  7. Treinamento Otimizado (train_model_improved):\n",
        "      * A fun√ß√£o train_model_improved (definida na c√©lula 5) √© chamada para iniciar o processo de treinamento real. Ela recebe o modelo, os data loaders de treino e valida√ß√£o, o n√∫mero de √©pocas, a taxa de aprendizado e o par√¢metro de patience para o early stopping.\n",
        "      * Esta fun√ß√£o gerencia todo o ciclo de treinamento: forward pass, c√°lculo de perda, backward pass, atualiza√ß√£o de pesos, avalia√ß√£o em valida√ß√£o e salvamento do melhor modelo.\n",
        "  8. Conclus√£o: Ap√≥s o t√©rmino da fun√ß√£o train_model_improved (seja por atingir o n√∫mero m√°ximo de √©pocas ou por early stopping), a c√©lula imprime \"üíæ Treinamento conclu√≠do!\" indicando que o processo principal foi finalizado.\n",
        "\n",
        "Em resumo, esta c√©lula coordena todas as etapas para construir, treinar e otimizar seu modelo de segmenta√ß√£o de emo√ß√µes, usando as configura√ß√µes e fun√ß√µes que voc√™ definiu nas c√©lulas anteriores."
      ],
      "metadata": {
        "id": "bO3YQUehcaxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Treinamento principal\n",
        "print(\"=== üöÄ INICIANDO TREINAMENTO OTIMIZADO ===\")\n",
        "\n",
        "# CONFIGURA√á√ïES MELHORADAS\n",
        "bert_model_name = 'neuralmind/bert-base-portuguese-cased'\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "print(\"‚úÖ Tokenizer carregado\")\n",
        "\n",
        "dataset = EmotionDataset(data, tokenizer)\n",
        "print(\"‚úÖ Dataset criado\")\n",
        "\n",
        "# SPLIT TREINO/VALIDA√á√ÉO\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "print(f\"üìä Divis√£o: {train_size} treino, {val_size} valida√ß√£o\")\n",
        "\n",
        "# MODELO MELHORADO\n",
        "model = EmotionSegmentationModel(\n",
        "    bert_model_name=bert_model_name,\n",
        "    num_emotions=len(dataset.emotion_to_id),\n",
        "    hidden_size=256,\n",
        "    num_layers=2,\n",
        "    dropout=0.3\n",
        ").to(device)\n",
        "\n",
        "print(f\"üß† Modelo criado\")\n",
        "print(f\"üìê Par√¢metros trein√°veis: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "# TREINAMENTO OTIMIZADO\n",
        "train_losses, val_accuracies = train_model_improved(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    num_epochs=10,\n",
        "    learning_rate=0.0005,\n",
        "    patience=4\n",
        ")\n",
        "\n",
        "print(\"üíæ Treinamento conclu√≠do!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3amyFUQr5j6",
        "outputId": "283d8054-07aa-423c-8f36-c2b48ff9c2fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== üöÄ INICIANDO TREINAMENTO OTIMIZADO ===\n",
            "‚úÖ Tokenizer carregado\n",
            "üé≠ Mapeamento de emo√ß√µes:\n",
            "   NEUTRO -> 0\n",
            "   FELIZ -> 1\n",
            "   TRISTE -> 2\n",
            "   RAIVA -> 3\n",
            "   MEDO -> 4\n",
            "   SURPRESA -> 5\n",
            "üîÑ Processando dados...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processando: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 307/307 [00:00<00:00, 446.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset criado\n",
            "üìä Divis√£o: 245 treino, 62 valida√ß√£o\n",
            "üß† Carregando BERT com fine-tuning...\n",
            "üß† Modelo criado\n",
            "üìê Par√¢metros trein√°veis: 18,577,414\n",
            "üöÄ Iniciando treinamento com 10 √©pocas...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "√âpoca 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [02:42<00:00,  5.25s/it, perda=1.1169]\n",
            "Validando: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:23<00:00,  2.92s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä √âpoca 1 - Perda: 1.6604, Acur√°cia Val: 0.4632\n",
            "üíæ Melhor modelo salvo (acur√°cia: 0.4632)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "√âpoca 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [02:32<00:00,  4.93s/it, perda=1.0208]\n",
            "Validando: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:23<00:00,  2.91s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä √âpoca 2 - Perda: 1.2439, Acur√°cia Val: 0.5838\n",
            "üíæ Melhor modelo salvo (acur√°cia: 0.5838)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "√âpoca 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [02:33<00:00,  4.96s/it, perda=0.4346]\n",
            "Validando: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:25<00:00,  3.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä √âpoca 3 - Perda: 0.9729, Acur√°cia Val: 0.6586\n",
            "üíæ Melhor modelo salvo (acur√°cia: 0.6586)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "√âpoca 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [02:34<00:00,  4.99s/it, perda=0.9299]\n",
            "Validando: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:23<00:00,  2.90s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä √âpoca 4 - Perda: 0.8162, Acur√°cia Val: 0.6508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "√âpoca 5/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [02:32<00:00,  4.93s/it, perda=0.7674]\n",
            "Validando: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:23<00:00,  2.90s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä √âpoca 5 - Perda: 0.7010, Acur√°cia Val: 0.6962\n",
            "üíæ Melhor modelo salvo (acur√°cia: 0.6962)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "√âpoca 6/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [02:33<00:00,  4.95s/it, perda=0.1023]\n",
            "Validando: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:22<00:00,  2.81s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä √âpoca 6 - Perda: 0.4976, Acur√°cia Val: 0.6768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "√âpoca 7/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [02:36<00:00,  5.05s/it, perda=0.4959]\n",
            "Validando: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:23<00:00,  2.90s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä √âpoca 7 - Perda: 0.5155, Acur√°cia Val: 0.7061\n",
            "üíæ Melhor modelo salvo (acur√°cia: 0.7061)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "√âpoca 8/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [02:32<00:00,  4.93s/it, perda=0.8913]\n",
            "Validando: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:23<00:00,  2.92s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä √âpoca 8 - Perda: 0.4793, Acur√°cia Val: 0.6685\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "√âpoca 9/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [02:33<00:00,  4.94s/it, perda=0.0763]\n",
            "Validando: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:23<00:00,  2.89s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä √âpoca 9 - Perda: 0.4257, Acur√°cia Val: 0.7117\n",
            "üíæ Melhor modelo salvo (acur√°cia: 0.7117)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "√âpoca 10/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 31/31 [02:32<00:00,  4.92s/it, perda=0.3569]\n",
            "Validando: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:23<00:00,  2.89s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä √âpoca 10 - Perda: 0.3063, Acur√°cia Val: 0.7117\n",
            "‚úÖ Melhor modelo carregado (acur√°cia: 0.7117)\n",
            "üíæ Treinamento conclu√≠do!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **7¬™ c√©lula**\n",
        "\n",
        "Teste de demonstra√ß√£o autom√°tico: possui algumas senten√ßas de teste para ser executado manualmente."
      ],
      "metadata": {
        "id": "ijlVDYcRcyWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Teste de demonstra√ß√£o\n",
        "def demonstrar_modelo_melhorado():\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üé≠ ANALISADOR DE EMO√á√ïES - VERS√ÉO MELHORADA\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"‚úÖ Modelo otimizado carregado!\")\n",
        "    print(\"üí° Digite frases em portugu√™s para an√°lise\")\n",
        "    print(\"üö™ Digite 'sair' para encerrar\")\n",
        "    print(\"-\"*60)\n",
        "\n",
        "    while True:\n",
        "        frase = input(\"\\nüìù Digite uma frase: \").strip()\n",
        "\n",
        "        if frase.lower() in ['sair', 'exit', 'quit']:\n",
        "            print(\"üëã At√© logo!\")\n",
        "            break\n",
        "\n",
        "        if not frase:\n",
        "            continue\n",
        "\n",
        "        print(\"‚è≥ Analisando com p√≥s-processamento inteligente...\")\n",
        "        segmentos = predict_emotion_segments_improved(\n",
        "            model, tokenizer, frase, dataset.id_to_emotion,\n",
        "            min_confidence=0.4\n",
        "        )\n",
        "\n",
        "        print(f\"\\n‚úÖ RESULTADO: '{frase}'\")\n",
        "        if not segmentos:\n",
        "            print(\"   ü§î Nenhum segmento emocional detectado com confian√ßa suficiente\")\n",
        "        else:\n",
        "            print(\"üé≠ Segmentos emocionais detectados:\")\n",
        "            for i, seg in enumerate(segmentos, 1):\n",
        "                emoji = {\n",
        "                    'FELIZ': 'üòä', 'TRISTE': 'üò¢', 'RAIVA': 'üò†',\n",
        "                    'MEDO': 'üò®', 'SURPRESA': 'üò≤', 'NEUTRO': 'üòê'\n",
        "                }.get(seg['emotion'], '‚ùì')\n",
        "                conf_str = f\"(conf: {seg['confidence']:.2f})\" if 'confidence' in seg else \"\"\n",
        "                print(f\"   {i}. {emoji} [{seg['emotion']}] {conf_str} ‚Üí '{seg['text']}'\")\n",
        "\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "# TESTAR COM EXEMPLOS PR√â-DEFINIDOS PRIMEIRO\n",
        "print(\"\\nüß™ TESTANDO COM EXEMPLOS:\")\n",
        "exemplos_teste = [\n",
        "    \"Estou muito feliz com a not√≠cia, mas depois fiquei triste ao pensar nas consequ√™ncias.\",\n",
        "    \"O filme foi assustador no in√≠cio, mas terminou de forma surpreendente e feliz.\",\n",
        "    \"Estou com raiva dessa situa√ß√£o, mas vou tentar manter a calma.\",\n",
        "    \"Que susto! Tive um grande medo quando ouvi aquele barulho.\",\n",
        "    \"Comecei uma caminhada na praia chorando pelo ocorrido, por√©m me alegrou ver as pessoas jogando bola e passeando com os cachorros.\",\n",
        "    \"Uma mulher muito bonita me abordou na entrada solicitando meus documentos, e fiquei indignada por n√£o terem me avisado dessa exig√™ncia antes.\"\n",
        "]\n",
        "\n",
        "for frase in exemplos_teste:\n",
        "    print(f\"\\nüìù Frase: {frase}\")\n",
        "    segmentos = predict_emotion_segments_improved(model, tokenizer, frase, dataset.id_to_emotion)\n",
        "    for i, seg in enumerate(segmentos, 1):\n",
        "        emoji = {'FELIZ': 'üòä', 'TRISTE': 'üò¢', 'RAIVA': 'üò†',\n",
        "                'MEDO': 'üò®', 'SURPRESA': 'üò≤', 'NEUTRO': 'üòê'}.get(seg['emotion'], '‚ùì')\n",
        "        print(f\"   {i}. {emoji} [{seg['emotion']}] ‚Üí '{seg['text']}'\")\n",
        "\n",
        "# INICIAR MODO INTERATIVO\n",
        "#demonstrar_modelo_melhorado()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCvqMaWNsChZ",
        "outputId": "f7ffa44f-3e11-485e-c5e2-9511c4392d04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üß™ TESTANDO COM EXEMPLOS:\n",
            "\n",
            "üìù Frase: Estou muito feliz com a not√≠cia, mas depois fiquei triste ao pensar nas consequ√™ncias.\n",
            "   1. üòä [FELIZ] ‚Üí 'Estou muito feliz com a not√≠cia'\n",
            "   2. üò¢ [TRISTE] ‚Üí ', mas depois fiquei triste ao pensar nas consequ√™ncias .'\n",
            "\n",
            "üìù Frase: O filme foi assustador no in√≠cio, mas terminou de forma surpreendente e feliz.\n",
            "   1. üòä [FELIZ] ‚Üí 'O filme'\n",
            "   2. üò≤ [SURPRESA] ‚Üí 'foi assustador'\n",
            "   3. üòä [FELIZ] ‚Üí 'no in√≠cio , mas terminou de forma surpreendente e feliz .'\n",
            "\n",
            "üìù Frase: Estou com raiva dessa situa√ß√£o, mas vou tentar manter a calma.\n",
            "   1. üò† [RAIVA] ‚Üí 'Estou com raiva dessa situa√ß√£o'\n",
            "   2. üòê [NEUTRO] ‚Üí ', mas vou tentar manter a calma .'\n",
            "\n",
            "üìù Frase: Que susto! Tive um grande medo quando ouvi aquele barulho.\n",
            "   1. üò® [MEDO] ‚Üí 'Que susto ! Tive um grande medo quando ouvi aquele barulho .'\n",
            "\n",
            "üìù Frase: Comecei uma caminhada na praia chorando pelo ocorrido, por√©m me alegrou ver as pessoas jogando bola e passeando com os cachorros.\n",
            "   1. üò† [RAIVA] ‚Üí 'Comecei'\n",
            "   2. üòä [FELIZ] ‚Üí 'uma'\n",
            "   3. üò† [RAIVA] ‚Üí 'caminhada na praia chorando pelo ocorrido'\n",
            "   4. üòä [FELIZ] ‚Üí ', por√©m me alegrou ver as pessoas'\n",
            "   5. üòê [NEUTRO] ‚Üí 'jogando bola e passeando com os cachorros .'\n",
            "\n",
            "üìù Frase: Uma mulher muito bonita me abordou na entrada solicitando meus documentos, e fiquei indignada por n√£o terem me avisado dessa exig√™ncia antes.\n",
            "   1. üò† [RAIVA] ‚Üí 'Uma mulher muito bonita me abordou na entrada solicitando meus documentos , e fiquei indignada por n√£o terem me avisado dessa exig√™ncia antes .'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **8¬™ c√©lula (final)**\n",
        "\n",
        "Fun√ß√£o simples de teste interativo com o usu√°rio, para demonstra√ß√£o em tempo real. Solicita que o usu√°rio insira uma senten√ßa e faz a an√°lise de sentimento."
      ],
      "metadata": {
        "id": "I2VzeyGYdAMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C√âLULA FINAL - FUN√á√ÉO SIMPLES PARA TESTAR O MODELO\n",
        "\n",
        "def analisar_sentimento():\n",
        "    \"\"\"\n",
        "    Fun√ß√£o simples que recebe uma frase do usu√°rio e mostra a an√°lise de sentimentos\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"üé≠ ANALISADOR DE SENTIMENTOS\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Carregar modelo treinado (assumindo que j√° foi treinado)\n",
        "    try:\n",
        "        # Se voc√™ j√° treinou o modelo nesta sess√£o, use estas vari√°veis:\n",
        "        # model, tokenizer, dataset j√° devem estar definidos\n",
        "\n",
        "        print(\"‚úÖ Modelo carregado! Digite suas frases para an√°lise.\")\n",
        "        print(\"üí° Digite 'sair' para encerrar\")\n",
        "        print(\"-\"*50)\n",
        "\n",
        "        while True:\n",
        "            # Receber frase do usu√°rio\n",
        "            frase = input(\"\\nüìù Digite uma frase: \").strip()\n",
        "\n",
        "            if frase.lower() in ['sair', 'exit', 'quit']:\n",
        "                print(\"üëã At√© logo!\")\n",
        "                break\n",
        "\n",
        "            if not frase:\n",
        "                print(\"‚ö†Ô∏è Por favor, digite uma frase.\")\n",
        "                continue\n",
        "\n",
        "            # Fazer a an√°lise\n",
        "            print(\"‚è≥ Analisando...\")\n",
        "            segmentos = predict_emotion_segments_improved(\n",
        "            model, tokenizer, frase, dataset.id_to_emotion,\n",
        "            min_confidence=0.4)\n",
        "\n",
        "            # Mostrar resultado\n",
        "            print(f\"\\n‚úÖ RESULTADO PARA: '{frase}'\")\n",
        "            print(\"üé≠ Segmentos emocionais detectados:\")\n",
        "\n",
        "            for i, segmento in enumerate(segmentos, 1):\n",
        "                emoji = {\n",
        "                    'FELIZ': 'üòä',\n",
        "                    'TRISTE': 'üò¢',\n",
        "                    'COM_RAIVA': 'üò†',\n",
        "                    'MEDO': 'üò®',\n",
        "                    'SURPRESA': 'üò≤',\n",
        "                    'NEUTRO': 'üòê'\n",
        "                }.get(segmento['emotion'], '‚ùì')\n",
        "\n",
        "                print(f\"   {i}. {emoji} [{segmento['emotion']}] ‚Üí '{segmento['text']}'\")\n",
        "\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "    except NameError:\n",
        "        print(\"‚ùå Modelo n√£o encontrado. Execute o treinamento primeiro!\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro: {e}\")\n",
        "\n",
        "analisar_sentimento()\n",
        "\n",
        "# PARA USAR:\n",
        "# 1. Primeiro execute todo o c√≥digo de treinamento\n",
        "# 2. Depois execute esta c√©lula\n",
        "# 3. Chame a fun√ß√£o: analisar_sentimento()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6QYqXwG2TkP",
        "outputId": "193f8bbb-8267-4a1b-eb5c-2991aaa31027"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "üé≠ ANALISADOR DE SENTIMENTOS\n",
            "==================================================\n",
            "‚úÖ Modelo carregado! Digite suas frases para an√°lise.\n",
            "üí° Digite 'sair' para encerrar\n",
            "--------------------------------------------------\n",
            "\n",
            "üìù Digite uma frase: As crian√ßas corriam pelo parque, rindo alto e espalhando folhas pelo ch√£o, at√© que o trov√£o estrondou e todos correram para se abrigar\n",
            "‚è≥ Analisando...\n",
            "\n",
            "‚úÖ RESULTADO PARA: 'As crian√ßas corriam pelo parque, rindo alto e espalhando folhas pelo ch√£o, at√© que o trov√£o estrondou e todos correram para se abrigar'\n",
            "üé≠ Segmentos emocionais detectados:\n",
            "   1. üòä [FELIZ] ‚Üí 'As crian√ßas corriam pelo parque , rindo alto e espalhando folhas pelo ch√£o'\n",
            "   2. üò® [MEDO] ‚Üí ', at√© que o trov√£o estrondou e todos correram para se abrigar'\n",
            "--------------------------------------------------\n",
            "\n",
            "üìù Digite uma frase: sair\n",
            "üëã At√© logo!\n"
          ]
        }
      ]
    }
  ]
}